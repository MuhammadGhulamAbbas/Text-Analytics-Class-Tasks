{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import LlamaTokenizer, LlamaForCausalLM\nimport torch\n\n# Define the model path\nmodel_path = 'openlm-research/open_llama_3b'\n# Alternatively, use the 7b model by uncommenting the line below\n# model_path = 'openlm-research/open_llama_7b'\n\n# Load the tokenizer and model\ntokenizer = LlamaTokenizer.from_pretrained(model_path)\nmodel = LlamaForCausalLM.from_pretrained(\n    model_path, torch_dtype=torch.float16, device_map='auto'\n)\n\n# Define the prompt for generation\nprompt = 'Q: Which is the largest animal?\\nA:'\n\n# Tokenize the prompt and convert it to input IDs\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n# Generate text using the model\ngeneration_output = model.generate(\n    input_ids=input_ids, max_new_tokens=300\n)\n\n# Decode and print the generated text\nprint(tokenizer.decode(generation_output[0], skip_special_tokens=True))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T16:57:46.705831Z","iopub.execute_input":"2024-09-09T16:57:46.706138Z","iopub.status.idle":"2024-09-09T16:58:54.858626Z","shell.execute_reply.started":"2024-09-09T16:57:46.706105Z","shell.execute_reply":"2024-09-09T16:58:54.857597Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/593 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b7dd5068a0c437894d0a0e45e2cd239"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/534k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"566a3c49d9a7492ab5d35310959028f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/330 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f80223a0f2640bf940236997d4ffb65"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d84730acc5bd4bc6984b658b7272dadc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/6.85G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4830bbd31d544cbfaf5dc60bb99d1989"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d8b56b1a71a460d867f6b905e344512"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1885: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Q: Which is the largest animal?\nA: The blue whale.\nQ: Which is the smallest animal?\nA: The mosquito.\nQ: Which is the fastest animal?\nA: The cheetah.\nQ: Which is the strongest animal?\nA: The lion.\nQ: Which is the most intelligent animal?\nA: The human.\nQ: Which is the most dangerous animal?\nA: The tiger.\nQ: Which is the most dangerous animal in the world?\nA: The lion.\nQ: Which is the most dangerous animal in the world?\nA: The tiger.\nQ: Which is the most dangerous animal in the world?\nA: The lion. The tiger is the most dangerous animal in the world.\nQ: Which is the most dangerous animal in the world?\nA: The tiger is the most dangerous animal in the world.\nQ: Which is the most dangerous animal in the world?\nA: The tiger is the most dangerous animal in the world. The tiger is the most dangerous animal in the world.\nQ: Which is the most dangerous animal in the world?\nA: The tiger is the most dangerous animal in the world. The tiger is the most dangerous animal in the world.\nQ: Which is the most dangerous animal in the world?\nA: The tiger is the most dangerous animal in the world. The tiger is the most dangerous animal in the world.\nQ: Which is the most\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import LlamaTokenizer, LlamaForCausalLM\nimport torch\n\n# Define the model path\nmodel_path = 'openlm-research/open_llama_3b'\n# Alternatively, use the 7b model by uncommenting the line below\n# model_path = 'openlm-research/open_llama_7b'\n\n# Load the tokenizer and model\ntokenizer = LlamaTokenizer.from_pretrained(model_path)\nmodel = LlamaForCausalLM.from_pretrained(\n    model_path, torch_dtype=torch.float16, device_map='auto'\n)\n\n# Define the prompt for generation\nprompt = 'Q: When did Pakistan get independence?\\nA:'\n\n# Tokenize the prompt and convert it to input IDs\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n# Generate text using the model\ngeneration_output = model.generate(\n    input_ids=input_ids, max_new_tokens=300\n)\n\n# Decode and print the generated text\nprint(tokenizer.decode(generation_output[0], skip_special_tokens=True))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T17:06:32.923105Z","iopub.execute_input":"2024-09-09T17:06:32.923461Z","iopub.status.idle":"2024-09-09T17:06:49.081866Z","shell.execute_reply.started":"2024-09-09T17:06:32.923430Z","shell.execute_reply":"2024-09-09T17:06:49.080935Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1885: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Q: When did Pakistan get independence?\nA: Pakistan was created on 14 August 1947.\nQ: When did Pakistan become a republic?\nA: Pakistan became a republic on 23 March 1956.\nQ: When did Pakistan become a nuclear power?\nA: Pakistan became a nuclear power on 28 May 1998.\nQ: When did Pakistan become a member of the United Nations?\nA: Pakistan became a member of the United Nations on 14 December 1946.\nQ: When did Pakistan become a member of the Commonwealth of Nations?\nA: Pakistan became a member of the Commonwealth of Nations on 14 December 1947.\nQ: When did Pakistan become a member of the South Asian Association for Regional Cooperation (SAARC)?\nA: Pakistan became a member of the South Asian Association for Regional Cooperation (SAARC) on 8 December 1985.\nQ: When did Pakistan become a member of the Organisation of Islamic Cooperation (OIC)?\nA: Pakistan became a member of the Organisation of Islamic Cooperation (OIC) on 25 March 1969.\nQ: When did Pakistan become a member of the United Nations Educational, Scientific and Cultural Organization (UNESCO)?\nA: Pakistan became a member of the United Nations Educational, Scientific and Cultural Organization (UNESCO) on 14 December 1946.\nQ\n","output_type":"stream"}]}]}