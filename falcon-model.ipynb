{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\n# Define the model path\nmodel_path = \"tiiuae/falcon-7b-instruct\"  # Technology Innovation Institute (UAE)\n\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path, torch_dtype=torch.float16, device_map='auto', trust_remote_code=True\n)\n\n# Define the prompt for generation\nprompt = 'Q: Which is the largest animal?\\nA:'\n\n# Tokenize the prompt and convert it to input IDs\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n# Generate text using the model\ngeneration_output = model.generate(\n    input_ids=input_ids, max_new_tokens=32\n)\n\n# Decode and print the generated text\nprint(tokenizer.decode(generation_output[0], skip_special_tokens=True))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-09T17:35:37.951949Z","iopub.execute_input":"2024-09-09T17:35:37.952228Z","iopub.status.idle":"2024-09-09T17:37:40.488372Z","shell.execute_reply.started":"2024-09-09T17:35:37.952196Z","shell.execute_reply":"2024-09-09T17:37:40.487390Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed0f94b9535b4c0c90892d7ef6261a0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.73M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9dfbe8e0f5fc4404841f9c01d235eede"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/281 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e952d4ac6c7d4dbb89c38ffd6cf873c8"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12f15e82d2ac4c11b6eab0fc81463eda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_falcon.py:   0%|          | 0.00/7.16k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7922fe522879453caae4e5ee1cc15270"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b-instruct:\n- configuration_falcon.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_falcon.py:   0%|          | 0.00/56.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bb9810b4ce7483c938b87a698aabd92"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b-instruct:\n- modeling_falcon.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin.index.json:   0%|          | 0.00/16.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba36b75b6ec84c86aeac4dcb640d3167"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f26833e63d8d4e5098c3343a78284c01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b88c8610c1654ce78beb58ad90fdd678"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00002.bin:   0%|          | 0.00/4.48G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f4e77de6feb465db3e238a53b515a82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21ee553630c6482cb65c7808e778959a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/117 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a37ddc430bcf45c49f8cfea122471dca"}},"metadata":{}},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1885: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Q: Which is the largest animal?\nA: The blue whale is the largest animal in the world.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\n# Define the model path\nmodel_path = \"tiiuae/falcon-7b-instruct\"  # Technology Innovation Institute (UAE)\n\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path, torch_dtype=torch.float16, device_map='auto', trust_remote_code=True\n)\n\n# Define the prompt for generation\nprompt = 'Q: When did Pakistan get independence?\\nA:'\n\n# Tokenize the prompt and convert it to input IDs\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n# Generate text using the model\ngeneration_output = model.generate(\n    input_ids=input_ids, max_new_tokens=300\n)\n\n# Decode and print the generated text\nprint(tokenizer.decode(generation_output[0], skip_special_tokens=True))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T17:38:48.065375Z","iopub.execute_input":"2024-09-09T17:38:48.066283Z","iopub.status.idle":"2024-09-09T17:39:06.121360Z","shell.execute_reply.started":"2024-09-09T17:38:48.066242Z","shell.execute_reply":"2024-09-09T17:39:06.120390Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"563e1b90b3324b85a367879d55bfbdcc"}},"metadata":{}},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1885: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Q: When did Pakistan get independence?\nA: Pakistan gained independence from British India on August 14, 1947.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\n# Define the model path\nmodel_path = \"tiiuae/falcon-7b-instruct\"  # Technology Innovation Institute (UAE)\n\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path, torch_dtype=torch.float16, device_map='auto', trust_remote_code=True\n)\n\n# Define the text and query for generation\ntext = \"Karachi, Islamabad and Lahore are few of the major cities in Pakistan.\"\nquery = \"Identify the entities in the following text: \"\nprompt = query + \"\\n\" + text\n\n# Tokenize the prompt and convert it to input IDs\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n# Generate text using the model\ngeneration_output = model.generate(\n    input_ids=input_ids, max_new_tokens=300\n)\n\n# Decode and print the generated text\nprint(tokenizer.decode(generation_output[0], skip_special_tokens=True))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T17:40:26.657038Z","iopub.execute_input":"2024-09-09T17:40:26.657662Z","iopub.status.idle":"2024-09-09T17:41:21.811096Z","shell.execute_reply.started":"2024-09-09T17:40:26.657606Z","shell.execute_reply":"2024-09-09T17:41:21.810093Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f273694814e47f9a51931d5add832ca"}},"metadata":{}},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1885: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Identify the entities in the following text: \nKarachi, Islamabad and Lahore are few of the major cities in Pakistan.\nEntities:\n- Karachi\n- Islamabad\n- Lahore\n- Pakistan\n","output_type":"stream"}]}]}