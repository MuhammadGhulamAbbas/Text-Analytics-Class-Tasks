{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Explicitly setting pad token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living\n",
      "in a remote, previously unexplored valley, in the Andes Mountains.\n",
      "Even more surprising to the researchers was the fact that the unicorns\n",
      "spoke perfect English.   \"The unicorns were very intelligent, and they\n",
      "were very intelligent,\" said Dr. David S. Siegel, a professor of\n",
      "anthropology at the University of California, Berkeley. \"They were\n",
      "very intelligent, and they were very intelligent, and they were very\n",
      "intelligent, and they were very intelligent, and they were very\n",
      "intelligent, and they were very intelligent, and they were very\n",
      "intelligent, and they were very 100\n"
     ]
    }
   ],
   "source": [
    "max_length = 128\n",
    "input_txt = \"\"\"In a shocking finding, scientist discovered \\\n",
    "a herd of unicorns living in a remote, previously unexplored \\\n",
    "valley, in the Andes Mountains. Even more surprising to the \\\n",
    "researchers was the fact that the unicorns spoke perfect English.\\n\\n\n",
    "\"\"\"\n",
    "input_ids = tokenizer(input_txt, padding=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "output_greedy = model.generate(input_ids, max_length=max_length, do_sample=False)\n",
    "print(textwrap.fill(tokenizer.decode(output_greedy[0])), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living\n",
      "in a remote, previously unexplored valley, in the Andes Mountains.\n",
      "Even more surprising to the researchers was the fact that the unicorns\n",
      "spoke perfect English.   The researchers, from the University of\n",
      "California, San Diego, and the University of California, Santa Cruz,\n",
      "found that the unicorns were able to communicate with each other in a\n",
      "way that was similar to that of human speech.   \"The unicorns were\n",
      "able to communicate with each other in a way that was similar to that\n",
      "of human speech,\" said study co-lead author Dr. David J. 100\n"
     ]
    }
   ],
   "source": [
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=5, do_sample=False)\n",
    "print(textwrap.fill(tokenizer.decode(output_beam[0])), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living\n",
      "in a remote, previously unexplored valley, in the Andes Mountains.\n",
      "Even more surprising to the researchers was the fact that the unicorns\n",
      "spoke perfect English.   The researchers, from the University of\n",
      "California, San Diego, and the National Science Foundation (NSF) in\n",
      "Boulder, Colorado, were able to translate the words of the unicorn\n",
      "into English, which they then translated into Spanish.  \"This is the\n",
      "first time that we have translated a language into an English\n",
      "language,\" said study co-author and NSF professor of linguistics and\n",
      "evolutionary biology Dr. 100\n"
     ]
    }
   ],
   "source": [
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=5, do_sample=False, no_repeat_ngram_size=2)\n",
    "print(textwrap.fill(tokenizer.decode(output_beam[0])), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living\n",
      "in a remote, previously unexplored valley, in the Andes Mountains.\n",
      "Even more surprising to the researchers was the fact that the unicorns\n",
      "spoke perfect English.   An archaeotherms of Peru says you'll never\n",
      "stop hearing new noises and noises, but just after being driven off a\n",
      "trail near the Orane Mountains, they spotted something completely\n",
      "alien.   Somehow humans came home together after a hunting game they\n",
      "played (that is one of the few examples where a human actually used\n",
      "that field field field):  But that will take an eternity or worseâ€¦.\n",
      "but 100\n"
     ]
    }
   ],
   "source": [
    "output_temp = model.generate(input_ids, max_length=max_length, do_sample=True, temperature=2.0, top_k=50)\n",
    "print(textwrap.fill(tokenizer.decode(output_temp[0])), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living\n",
      "in a remote, previously unexplored valley, in the Andes Mountains.\n",
      "Even more surprising to the researchers was the fact that the unicorns\n",
      "spoke perfect English.   When researchers brought the unicorns to\n",
      "Bolivia, they found that they spoke the same vocabulary. This finding\n",
      "is quite shocking. First of all, the scientists knew nothing about the\n",
      "ancient language of the natives of Peru. Secondly, the unicorns are\n",
      "quite similar to the traditional Latin-language spoken by people of\n",
      "the Amazon region. And yet the researchers discovered that the\n",
      "unicorns themselves did not have a specific language. 100\n"
     ]
    }
   ],
   "source": [
    "output_topp = model.generate(input_ids, max_length=max_length, do_sample=True, top_p=0.90)\n",
    "print(textwrap.fill(tokenizer.decode(output_topp[0])), 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLaMA based Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Q: Which is the largest animal?\n",
      "A: The blue whale.\n",
      "Q: Which is the smallest animal?\n",
      "A: The mosquito.\n",
      "Q: Which is the fastest animal?\n",
      "A: The cheetah.\n",
      "Q: Which is the strongest animal?\n",
      "A: The lion.\n",
      "Q: Which is the most intelligent animal?\n",
      "A: The human.\n",
      "Q: Which is the most dangerous animal?\n",
      "A: The tiger.\n",
      "Q: Which is the most dangerous animal in the world?\n",
      "A: The lion.\n",
      "Q: Which is the most dangerous animal in the world?\n",
      "A: The tiger.\n",
      "Q: Which is the most dangerous animal in the world?\n",
      "A: The lion. The tiger is the most dangerous animal in the world.\n",
      "Q: Which is the most dangerous animal in the world?\n",
      "A: The tiger is the most dangerous animal in the world.\n",
      "Q: Which is the most dangerous animal in the world?\n",
      "A: The tiger is the most dangerous animal in the world. The tiger is the most dangerous animal in the world.\n",
      "Q: Which is the most dangerous animal in the world?\n",
      "A: The tiger is the most dangerous animal in the world. The tiger is the most dangerous animal in the world.\n",
      "Q: Which is the most dangerous animal in the world?\n",
      "A: The tiger is the most dangerous animal in the world. The tiger is the most dangerous animal in the world.\n",
      "Q: Which is the most\n"
     ]
    }
   ],
   "source": [
    "model_path = 'openlm-research/open_llama_3b'\n",
    "# model_path = 'openlm-research/open_llama_7b'\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_path, torch_dtype=torch.float16, device_map='auto',\n",
    ")\n",
    "prompt = 'Q: Which is the largest animal?\\nA:'\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "generation_output = model.generate(\n",
    "    input_ids=input_ids, max_new_tokens=300\n",
    ")\n",
    "print(tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Q: When did Pakistan get independence?\\nA:'\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "generation_output = model.generate(\n",
    "    input_ids=input_ids, max_new_tokens=300\n",
    ")\n",
    "print(tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Falcon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"tiiuae/falcon-7b-instruct\" #Technology Innovation Institute (UAE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, torch_dtype=torch.float16, device_map='auto',trust_remote_code=True)\n",
    "prompt = 'Q: Which is the largest animal?\\nA:'\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "generation_output = model.generate(input_ids=input_ids, max_new_tokens=32)\n",
    "print(tokenizer.decode(generation_output[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Q: When did Pakistan get independence?\\nA:'\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "generation_output = model.generate(\n",
    "    input_ids=input_ids, max_new_tokens=300)\n",
    "print(tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Karachi, Islamabad and Lahore are few of the major cities in Pakistan.\"\n",
    "query = \"identify the entities in the following text: \"\n",
    "prompt = query + \"\\n\" + text\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "generation_output = model.generate(\n",
    "    input_ids=input_ids, max_new_tokens=300)\n",
    "print(tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Dear Amazon, last week I ordered an Optimus Prime action figure from your online store in Germany. \n",
    "Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead! \n",
    "As a lifelong enemy of the Decepticons, I hope you can understand my dilemma. \n",
    "To resolve the issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered. \n",
    "Enclosed are copies of my records concerning this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"identify the entities and their types (use standard NER terminology) in the following text: \"\n",
    "prompt = query + \"\\n\" + text\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "generation_output = model.generate(\n",
    "    input_ids=input_ids, max_new_tokens=800)\n",
    "print(tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"summarize the following text: \"\n",
    "prompt = query + \"\\n\" + text\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "generation_output = model.generate(\n",
    "    input_ids=input_ids, max_new_tokens=600)\n",
    "print(tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Describe the sentiment of the following text as either positive, negative or neutral: \"\n",
    "prompt = query + \"\\n\" + text\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "generation_output = model.generate(\n",
    "    input_ids=input_ids, max_new_tokens=600)\n",
    "print(tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = \"Dear Bumblebee, I am sorry to hear that your order was mixed up. \"\n",
    "prompt = text + \"\\n\\nCustomer service response:\\n\" + response\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "generation_output = model.generate(\n",
    "    input_ids=input_ids, max_new_tokens=900)\n",
    "print(tokenizer.decode(generation_output[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
