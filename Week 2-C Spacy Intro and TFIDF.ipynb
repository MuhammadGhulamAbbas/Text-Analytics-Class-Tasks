{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-25 10:55:24.193984: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-25 10:55:24.221328: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-25 10:55:24.221361: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-25 10:55:24.221376: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-25 10:55:24.227681: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-25 10:55:25.822289: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-25 10:55:25.850411: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-25 10:55:25.850447: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy import displacy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Pakistan\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " got independence in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1947\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ". \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Karachi\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Lahore\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Islamabad\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " are few of the major cities of \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Pakistan\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_sent = \"Pakistan got independence in 1947. Karachi, Lahore and Islamabad are few of the major cities of Pakistan.\"\n",
    "\n",
    "parsed_sent = nlp(test_sent)\n",
    "spacy.displacy.render(parsed_sent, style='ent',jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            text            lemma    pos  tag    dep is_stop\n",
      "0       Pakistan         Pakistan  PROPN  NNP  nsubj   False\n",
      "1            got           (get,)   VERB  VBD   ROOT   False\n",
      "2   independence  (independence,)   NOUN   NN   dobj   False\n",
      "3             in            (in,)    ADP   IN   prep    True\n",
      "4           1947          (1947,)    NUM   CD   pobj   False\n",
      "5              .             (.,)  PUNCT    .  punct   False\n",
      "6        Karachi       (Karachi,)  PROPN  NNP  nsubj   False\n",
      "7              ,             (,,)  PUNCT    ,  punct   False\n",
      "8         Lahore        (Lahore,)  PROPN  NNP   conj   False\n",
      "9            and           (and,)  CCONJ   CC     cc    True\n",
      "10     Islamabad     (Islamabad,)  PROPN  NNP   conj   False\n",
      "11           are            (be,)    AUX  VBP   ROOT    True\n",
      "12           few           (few,)    ADJ   JJ   attr    True\n",
      "13            of            (of,)    ADP   IN   prep    True\n",
      "14           the           (the,)    DET   DT    det    True\n",
      "15         major         (major,)    ADJ   JJ   amod   False\n",
      "16        cities          (city,)   NOUN  NNS   pobj   False\n",
      "17            of            (of,)    ADP   IN   prep    True\n",
      "18      Pakistan      (Pakistan,)  PROPN  NNP   pobj   False\n",
      "19             .             (.,)  PUNCT    .  punct   False\n"
     ]
    }
   ],
   "source": [
    "df_token = pd.DataFrame()\n",
    "\n",
    "for i, token in enumerate(parsed_sent):\n",
    "    df_token.loc[i, 'text'] = token.text\n",
    "    df_token.loc[i, 'lemma'] = token.lemma_,\n",
    "    df_token.loc[i, 'pos'] = token.pos_\n",
    "    df_token.loc[i, 'tag'] = token.tag_\n",
    "    df_token.loc[i, 'dep'] = token.dep_\n",
    "    #df_token.loc[i, 'shape'] = token.shape_\n",
    "    #df_token.loc[i, 'is_alpha'] = token.is_alpha\n",
    "    df_token.loc[i, 'is_stop'] = token.is_stop\n",
    "    \n",
    "print(df_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stabil\n",
      "destabil\n",
      "footbal\n",
      "studi\n",
      "studi\n",
      "beauti\n",
      "beauti\n"
     ]
    }
   ],
   "source": [
    "print(porter.stem(\"stabilize\"))\n",
    "print(porter.stem(\"destabilize\"))\n",
    "print(porter.stem(\"football\"))\n",
    "print(porter.stem(\"studies\"))\n",
    "print(porter.stem(\"studying\"))\n",
    "print(porter.stem(\"beautiful\"))\n",
    "print(porter.stem(\"beauty\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stablize stablize\n",
      "destablize destablize\n",
      "football football\n",
      "studies study\n",
      "studying study\n",
      "beautiful beautiful\n",
      "beauty beauty\n"
     ]
    }
   ],
   "source": [
    "test_token = \"stablize destablize football studies studying beautiful beauty\"\n",
    "parsed_sent = nlp(test_token)\n",
    "for token in parsed_sent:\n",
    "    print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'The author is writing a new book.'\n",
    "patterns = [{\"POS\": \"AUX\"}, {\"POS\": \"VERB\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is writing\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(sentence)\n",
    "lists = textacy.extract.token_matches(doc, patterns)\n",
    "for list in lists:\n",
    "    print(list.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will introduce\n",
      "will make\n"
     ]
    }
   ],
   "source": [
    "sentence2 = \"The talk will introduce reader about use cases of Natural Language Processing in \\\n",
    "            Fintech. It will make use of interesting examples along the way.\"\n",
    "doc = nlp(sentence2)\n",
    "lists = textacy.extract.token_matches(doc, patterns)\n",
    "for list in lists:\n",
    "    print(list.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The talk\n",
      "reader\n",
      "use\n",
      "cases\n",
      "Natural Language Processing\n",
      "Fintech\n",
      "It\n",
      "use\n",
      "interesting examples\n",
      "the way\n"
     ]
    }
   ],
   "source": [
    "for n in doc.noun_chunks:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Processing\n",
      "Fintech\n"
     ]
    }
   ],
   "source": [
    "abc3 = textacy.extract.basics.entities(doc)\n",
    "for a in abc3:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = \"My favorite dog is fluffy and tan\"\n",
    "d2 = \"The dog is brown and cat is brown\"\n",
    "d3 = \"My favorite hat is brown and coat is pink\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my favorite dog is fluffy and tan', 'the dog is brown and cat is brown', 'my favorite hat is brown and coat is pink']\n"
     ]
    }
   ],
   "source": [
    "docs = [d1.lower(), d2.lower(), d3.lower()]\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the', 'is', 'and', 'my'}\n",
      "['favorite', 'dog', 'fluffy', 'tan']\n",
      "['dog', 'brown', 'cat', 'brown']\n",
      "['favorite', 'hat', 'brown', 'coat', 'pink']\n",
      "['favorite dog fluffy tan', 'dog brown cat brown', 'favorite hat brown coat pink']\n"
     ]
    }
   ],
   "source": [
    "stopword = set(\"is and the my\".split())\n",
    "print(stopword)\n",
    "s = []\n",
    "for doc in docs: \n",
    "    s_list = [word for word in doc.split() if word not in stopword]\n",
    "    print(s_list)\n",
    "    str_ = ' '.join(s_list)   \n",
    "    s.append(str_) \n",
    "print(s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'favorite': 4, 'dog': 3, 'fluffy': 5, 'tan': 8, 'brown': 0, 'cat': 1, 'hat': 6, 'coat': 2, 'pink': 7}\n",
      "['brown' 'cat' 'coat' 'dog' 'favorite' 'fluffy' 'hat' 'pink' 'tan']\n"
     ]
    }
   ],
   "source": [
    "cnt_vectorizer = CountVectorizer(binary=True)\n",
    "cnt_vectorizer.fit(s)\n",
    "print(cnt_vectorizer.vocabulary_)\n",
    "print(cnt_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 9)\n",
      "[[0 0 0 1 1 1 0 0 1]\n",
      " [1 1 0 1 0 0 0 0 0]\n",
      " [1 0 1 0 1 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "vec1 = cnt_vectorizer.transform(s).toarray()\n",
    "print(vec1.shape)\n",
    "print(vec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brown' 'brown cat' 'brown coat' 'cat' 'cat brown' 'coat' 'coat pink'\n",
      " 'dog' 'dog brown' 'dog fluffy' 'favorite' 'favorite dog' 'favorite hat'\n",
      " 'fluffy' 'fluffy tan' 'hat' 'hat brown' 'pink' 'tan']\n"
     ]
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(binary=True,ngram_range=(1, 2))\n",
    "bigram_vectorizer.fit(s)\n",
    "print(bigram_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 19)\n",
      "[[0 0 0 0 0 0 0 1 0 1 1 1 0 1 1 0 0 0 1]\n",
      " [1 1 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 1 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "vec2 = bigram_vectorizer.transform(s).toarray()\n",
    "print(vec2.shape)\n",
    "print(vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.42804604 0.42804604 0.5628291\n",
      "  0.         0.         0.5628291 ]\n",
      " [0.77100584 0.50689001 0.         0.38550292 0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.37302199 0.         0.49047908 0.         0.37302199 0.\n",
      "  0.49047908 0.49047908 0.        ]]\n",
      "['brown' 'cat' 'coat' 'dog' 'favorite' 'fluffy' 'hat' 'pink' 'tan']\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words=['is', 'and', 'the', 'my']) #,binary=True)\n",
    "vec4 = tfidf_vectorizer.fit_transform(s).toarray()\n",
    "print(vec4)\n",
    "print(tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['Sebastian Thrun', 'self-driving cars', 'Google', 'few people', 'the company', 'him', 'I', 'you', 'very senior CEOs', 'major American car companies', 'my hand', 'I', 'Thrun', 'an interview', 'Recode']\n",
      "Verbs: ['start', 'work', 'drive', 'take', 'tell', 'shake', 'turn', 'talk', 'say']\n",
      "Sebastian Thrun PERSON\n",
      "Google ORG\n",
      "2007 DATE\n",
      "American NORP\n",
      "Thrun FAC\n",
      "Recode ORG\n",
      "earlier this week DATE\n"
     ]
    }
   ],
   "source": [
    "# Process whole documents\n",
    "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        \"Google in 2007, few people outside of the company took him \"\n",
    "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
    "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
    "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
    "        \"this week.\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# Analyze syntax\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple 0 5 ORG\n",
      "U.K. 27 31 GPE\n",
      "$1 billion 44 54 MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
